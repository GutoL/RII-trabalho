{
    "paper_authors_list": [
        "Ding, Caiwen", 
        "Liao, Siyu", 
        "Wang, Yanzhi", 
        "Li, Zhe", 
        "Liu, Ning", 
        "Zhuo, Youwei", 
        "Wang, Chao", 
        "Qian, Xuehai", 
        "Bai, Yu", 
        "Yuan, Geng", 
        "Ma, Xiaolong", 
        "Zhang, Yipeng", 
        "Tang, Jian", 
        "Qiu, Qinru", 
        "Lin, Xue", 
        "Yuan, Bo"
    ], 
    "paper_comments": "14 pages, 15 Figures, conference", 
    "paper_page_url": "https://arxiv.org/abs/1708.08917", 
    "paper_abstract": "Large-scale deep neural networks (DNNs) are both compute and memory\nintensive. As the size of DNNs continues to grow, it is critical to improve the\nenergy efficiency and performance while maintaining accuracy. For DNNs, the\nmodel size is an important factor affecting performance, scalability and energy\nefficiency. Weight pruning achieves good compression ratios but suffers from\nthree drawbacks: 1) the irregular network structure after pruning; 2) the\nincreased training complexity; and 3) the lack of rigorous guarantee of\ncompression ratio and inference accuracy. To overcome these limitations, this\npaper proposes CirCNN, a principled approach to represent weights and process\nneural networks using block-circulant matrices. CirCNN utilizes the Fast\nFourier Transform (FFT)-based fast multiplication, simultaneously reducing the\ncomputational complexity (both in inference and training) from O(n2) to\nO(nlogn) and the storage complexity from O(n2) to O(n), with negligible\naccuracy loss. Compared to other approaches, CirCNN is distinct due to its\nmathematical rigor: it can converge to the same effectiveness as DNNs without\ncompression. The CirCNN architecture, a universal DNN inference engine that can\nbe implemented on various hardware/software platforms with configurable network\narchitecture. To demonstrate the performance and energy efficiency, we test\nCirCNN in FPGA, ASIC and embedded processors. Our results show that CirCNN\narchitecture achieves very high energy efficiency and performance with a small\nhardware footprint. Based on the FPGA implementation and ASIC synthesis\nresults, CirCNN achieves 6-102X energy efficiency improvements compared with\nthe best state-of-the-art results.", 
    "paper_subjects": [
        "Learning (cs.LG)"
    ], 
    "paper_code": "1708.08917", 
    "paper_submission_date": "2017/08/29", 
    "paper_title": "CirCNN: Accelerating and Compressing Deep Neural Networks Using Block-CirculantWeight Matrices"
}