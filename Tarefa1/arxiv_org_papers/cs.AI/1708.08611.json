{
    "paper_authors_list": [
        "Alshiekh, Mohammed", 
        "Bloem, Roderick", 
        "Ehlers, Ruediger", 
        "K&#xf6;nighofer, Bettina", 
        "Niekum, Scott", 
        "Topcu, Ufuk"
    ], 
    "paper_comments": null, 
    "paper_page_url": "https://arxiv.org/abs/1708.08611", 
    "paper_abstract": "Reinforcement learning algorithms discover policies that maximize reward, but\ndo not necessarily guarantee safety during learning or execution phases. We\nintroduce a new approach to learn optimal policies while enforcing properties\nexpressed in temporal logic. To this end, given the temporal logic\nspecification that is to be obeyed by the learning system, we propose to\nsynthesize a reactive system called a shield. The shield is introduced in the\ntraditional learning process in two alternative ways, depending on the location\nat which the shield is implemented. In the first one, the shield acts each time\nthe learning agent is about to make a decision and provides a list of safe\nactions. In the second way, the shield is introduced after the learning agent.\nThe shield monitors the actions from the learner and corrects them only if the\nchosen action causes a violation of the specification. We discuss which\nrequirements a shield must meet to preserve the convergence guarantees of the\nlearner. Finally, we demonstrate the versatility of our approach on several\nchallenging reinforcement learning scenarios.", 
    "paper_subjects": [
        "Artificial Intelligence (cs.AI)", 
        "Learning (cs.LG)"
    ], 
    "paper_code": "1708.08611", 
    "paper_submission_date": "2017/08/29", 
    "paper_title": "Safe Reinforcement Learning via Shielding"
}