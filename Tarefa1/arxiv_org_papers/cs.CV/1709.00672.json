{
    "paper_authors_list": [
        "Pandey, Gaurav", 
        "Dukkipati, Ambedkar"
    ], 
    "paper_comments": "10 pages, 4 figures, International Conference on Data Mining, 2017", 
    "paper_page_url": "https://arxiv.org/abs/1709.00672", 
    "paper_abstract": "In recent years, deep discriminative models have achieved extraordinary\nperformance on supervised learning tasks, significantly outperforming their\ngenerative counterparts. However, their success relies on the presence of a\nlarge amount of labeled data. How can one use the same discriminative models\nfor learning useful features in the absence of labels? We address this question\nin this paper, by jointly modeling the distribution of data and latent features\nin a manner that explicitly assigns zero probability to unobserved data. Rather\nthan maximizing the marginal probability of observed data, we maximize the\njoint probability of the data and the latent features using a two step EM-like\nprocedure. To prevent the model from overfitting to our initial selection of\nlatent features, we use adversarial regularization. Depending on the task, we\nallow the latent features to be one-hot or real-valued vectors and define a\nsuitable prior on the features. For instance, one-hot features correspond to\nclass labels and are directly used for the unsupervised and semi-supervised\nclassification task, whereas real-valued feature vectors are fed as input to\nsimple classifiers for auxiliary supervised discrimination tasks. The proposed\nmodel, which we dub discriminative encoder (or DisCoder), is flexible in the\ntype of latent features that it can capture. The proposed model achieves\nstate-of-the-art performance on several challenging tasks.", 
    "paper_subjects": null, 
    "paper_code": "1709.00672", 
    "paper_submission_date": "2017/09/03", 
    "paper_title": "Unsupervised feature learning with discriminative encoder"
}