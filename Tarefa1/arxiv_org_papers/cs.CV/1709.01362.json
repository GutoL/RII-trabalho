{
    "paper_authors_list": [
        "Dong, Jianfeng", 
        "Li, Xirong", 
        "Snoek, Cees G. M."
    ], 
    "paper_comments": "10 pages, 7 figures, under view of TMM", 
    "paper_page_url": "https://arxiv.org/abs/1709.01362", 
    "paper_abstract": "This paper strives to find amidst a set of sentences the one best describing\nthe content of a given image or video. Different from existing works, which\nrely on a joint subspace for their image and video caption retrieval, we\npropose to do so in a visual space exclusively. Apart from this conceptual\nnovelty, we contribute \\emph{Word2VisualVec}, a deep neural network\narchitecture that learns to predict a visual feature representation from\ntextual input. Example captions are encoded into a textual embedding based on\nmulti-scale sentence vectorization and further transferred into a deep visual\nfeature of choice via a simple multi-layer perceptron. We further generalize\nWord2VisualVec for video caption retrieval, by predicting from text both 3-D\nconvolutional neural network features as well as a visual-audio representation.\nExperiments on Flickr8k, Flickr30k, the Microsoft Video Description dataset and\nthe very recent NIST TrecVid challenge for video caption retrieval detail\nWord2VisualVec's properties, its benefit over textual embeddings, the potential\nfor multimodal query composition and its state-of-the-art results.", 
    "paper_subjects": null, 
    "paper_code": "1709.01362", 
    "paper_submission_date": "2017/09/05", 
    "paper_title": "Predicting Visual Features from Text for Image and Video Caption Retrieval"
}